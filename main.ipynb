{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7bc685-bee5-4ddb-9bac-3333df26311c",
   "metadata": {},
   "source": [
    "# Clothes shopping recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ef798-516d-46f6-804e-8b46fbee2fde",
   "metadata": {},
   "source": [
    "This project aims to improve customer shopping enthusiasm and online shopping experiences. It details and explains the model used for recommending shop items that users have a high possibility to purchase and the comparison between several approaches to predicting similarities and thus recommendation. \\\n",
    "The experimental results show that by measuring user similarity, specifically between purchase and rating history, one is able to more precisely predict user ratings for items that have not yet been purchased by the user and coincidentally curate a list of (new) items that the user is highly likely to purchase and rate high.\\\n",
    "Results from this study are not limited to the fashion industry alone. The models can be used in many other industries where user-item ratings are available and rating predictions could influence item recommendation to the users. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "db8a15a8-49c2-481f-8481-ae4871e468c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "74265dfa-d60d-4ee6-b542-be26f21fcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "#from gensim.models import Word2Vec\n",
    "#from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412b036-7bca-4a28-8d93-dbedb237814a",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The dataset used contains the measurements of clothing fit from RentTheRunway. The data is cited from the report called Decomposing fit semantics for product size recommendation in metric spaces by Rishabh Misra, Mengting Wan, Julian McAuley RecSys, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a06f5956-6584-406a-a23c-6b5a42a5a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open('data/renttherunway_final_data.json.gz')\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(json.loads(l))\n",
    "    if len(dataset) == 100000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "107a805c-fd08-41cc-b1a1-2c7d57762a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "training_set = dataset[0:int(len(dataset)*9/10)]\n",
    "test_set = dataset[int(len(dataset)*9/10):]\n",
    "print(len(training_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5f6bf-f873-446f-8829-ebae7dc98662",
   "metadata": {},
   "source": [
    "This dataset contains a total of 192,544 example vectors and aside from body measurements such as bus size, height, and weight, it also includes the text feedback (review_text), review date from each itemâ€™s purchase/rental, and finally the rating given to the item by the user on a scale of 1-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "072fdfee-38dc-4d62-b9d7-431d6af5f435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit': 'fit',\n",
       " 'user_id': '420272',\n",
       " 'bust size': '34d',\n",
       " 'item_id': '2260466',\n",
       " 'weight': '137lbs',\n",
       " 'rating': '10',\n",
       " 'rented for': 'vacation',\n",
       " 'review_text': \"An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.\",\n",
       " 'body type': 'hourglass',\n",
       " 'review_summary': 'So many compliments!',\n",
       " 'category': 'romper',\n",
       " 'height': '5\\' 8\"',\n",
       " 'size': 14,\n",
       " 'age': '28',\n",
       " 'review_date': 'April 20, 2016'}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0d25da28-409d-467b-a724-6990f7a817e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 42\n",
      "1 0\n",
      "2 463\n",
      "3 0\n",
      "4 1344\n",
      "5 0\n",
      "6 4974\n",
      "7 0\n",
      "8 24852\n",
      "9 0\n",
      "10 58325\n"
     ]
    }
   ],
   "source": [
    "#count number of rating\n",
    "rating = [int(d['rating']) if d['rating'] is not None else 0 for d in training_set]\n",
    "for i in range(0,11):\n",
    "    print(i,rating.count(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a4422-1391-4d99-84dc-5637d4f8ec32",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6c860c3a-216d-4687-8b34-b547e230eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1 , s2):\n",
    "    numerator = len(s1.intersection(s2))\n",
    "    denominator = len(s1.union(s2))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "65ec1f90-2e36-46e5-8e76-ab48dce22b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict (set)\n",
    "itemsPerUser = defaultdict (set)\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "for d in training_set:\n",
    "    user,item = d['user_id'], d['item_id']\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    ratingDict[(user,item)] = d['rating']\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fb9e37dd-b45f-4a81-8dc9-4c6ba7e14cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilarUser(i): # Query item i, and number of results K to return\n",
    "    similarities = []\n",
    "    items = itemsPerUser[i] # items which were purchased u\n",
    "    for j in itemsPerUser : # Compute similarity against each item\n",
    "        if j == i: continue\n",
    "        sim = Jaccard(items , itemsPerUser[j])\n",
    "        similarities.append((sim ,j))\n",
    "        similarities.sort(reverse=True) # Sort to find the most similar\n",
    "    return similarities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "321e107c-b4e6-49cf-af30-0a853e77926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in training_set:\n",
    "    i = d['item_id']\n",
    "    u = d['user_id']\n",
    "    if d['rating'] is None:\n",
    "        d['rating'] = 0\n",
    "    else:\n",
    "        d['rating'] =  int(d['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "68d01ecb-2114-4a7b-b8c3-82497aad9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([int(d['rating']) if d['rating'] is not None else 0 for d in training_set]) / len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "da3ecaac-6c3d-4433-95bf-3c03c07b80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(item, user):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerItem[item]:\n",
    "        j = d['user_id']\n",
    "        if j == user: continue\n",
    "        ratings.append(d['rating'])\n",
    "        similarities.append(Jaccard(itemsPerUser[user], itemsPerUser[j]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x, y in zip(ratings, similarities)]\n",
    "        return sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "de6fe432-5285-4c08-9c82-8d3017184298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.091244444444444"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "db0503ec-d7a8-40fa-b561-ba48ce65bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "simPredictions = [predictRating(d['item_id'], d['user_id']) for d in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "28291fbf-0fbd-490d-ab67-9bc2421d21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(d['rating']) if d['rating'] is not None else 0 for d in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "687c54fa-2646-4ed6-91b9-ad136299ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions, labels)]\n",
    "    return sum(differences)/ len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "028ba905-c55f-4d31-8507-b1360fe622a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.13332842884137"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(simPredictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd86f8-d9ea-4885-bd61-02fea663ec74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9170b0e-1855-4300-a9fd-6d8966829098",
   "metadata": {},
   "source": [
    "### Bag of Words with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "50e5f3e3-a607-49fa-805e-49e3f4ed72f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76921"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total words\n",
    "wordCount = defaultdict(int)\n",
    "for d in training_set:\n",
    "    for w in d['review_text'].split():\n",
    "        wordCount[w] += 1\n",
    "len(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "de055ef9-2735-4bff-9ee2-e498f826891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total words after filter\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in training_set:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "be02d6d6-5b4c-4482-9bb0-cbfae9ef09a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32537"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "19123ae0-04cc-49d9-abc0-62351c1da531",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "for d in training_set:\n",
    "    if d['rating'] is not None:\n",
    "        training.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "48435001-5a93-42f1-9751-b10b6324b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for d in test_set:\n",
    "    if d['rating'] is not None:\n",
    "        test.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "600bc6ec-40da-4903-8a68-7a82ecfa8d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.091244444444444\n"
     ]
    }
   ],
   "source": [
    "#mean of total rating\n",
    "ratingMean = sum([int(d['rating']) if d['rating'] is not None else 0 for d in training_set]) / len(training_set)\n",
    "print(ratingMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2cd4a937-8a57-4a2e-9203-acd5df476a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemAverages = defaultdict(list)\n",
    "reviewsPerUser = defaultdict(list)\n",
    "userAverages = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "for d in training:\n",
    "    i = d['item_id']\n",
    "    u = d['user_id']\n",
    "    if d['rating'] is None:\n",
    "        d['rating'] = 0\n",
    "    else:\n",
    "        d['rating'] =  int(d['rating'])\n",
    "    itemAverages[i].append(d['rating'])\n",
    "    reviewsPerUser[u].append(d)\n",
    "    \n",
    "    userAverages[u].append(d['rating'])\n",
    "    reviewsPerItem[i].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1be7db4e-e902-4339-b596-7d56c5be7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsPerUser = defaultdict(list)\n",
    "for d in training:\n",
    "    i = d['item_id']\n",
    "    u = d['user_id']\n",
    "    itemsPerUser[u].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "23c4cf36-c396-4aec-b9e6-3d0bd76995e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(list)\n",
    "for d in training:\n",
    "    i = d['item_id']\n",
    "    u = d['user_id']\n",
    "    usersPerItem[i].append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3cd14166-c835-4955-ad34-49976f568daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in itemAverages:\n",
    "    itemAverages[i] = sum(itemAverages[i]) / len(itemAverages[i])\n",
    "    \n",
    "for u in userAverages:\n",
    "    userAverages[u] = sum(userAverages[u]) / len(userAverages[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "63061163-e559-4ae9-a8c3-df914698dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f2e6e5ac-2310-405a-b919-5e75498ac04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join all the review\n",
    "wordsPerUser = defaultdict(list)\n",
    "wordsPerItem = defaultdict(list)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in training_set:\n",
    "    u = d['user_id'] \n",
    "    i = d['item_id']\n",
    "    wordsPerUser[u].append(''.join([c for c in d['review_text'].lower() if not c in punctuation]))\n",
    "    wordsPerItem[i].append(''.join([c for c in d['review_text'].lower() if not c in punctuation]))\n",
    "    #wordsPerUser[u].append(d['review_text'])\n",
    "for i in wordsPerUser:\n",
    "    wordsPerUser[i] = ' '.join(wordsPerUser[i])\n",
    "for i in wordsPerItem:\n",
    "    wordsPerItem[i] = ' '.join(wordsPerItem[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b84a5d8a-d12e-4583-9e13-68379bf56c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine(x1,x2):\n",
    "    numer = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0\n",
    "    for a1,a2 in zip(x1,x2):\n",
    "        numer += a1*a2\n",
    "        norm1 += a1**2\n",
    "        norm2 += a2**2\n",
    "    if norm1*norm2:\n",
    "        return numer / math.sqrt(norm1*norm2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c7e487a4-570a-408a-98c2-75f7db7338a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in test:\n",
    "    i = d['item_id']\n",
    "    u = d['user_id']\n",
    "    if d['rating'] is None:\n",
    "        print('None element')\n",
    "        d['rating'] = 0\n",
    "    else:\n",
    "        d['rating'] =  int(d['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f02e5579-7043-4895-96fe-44dfb5ce2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarities by user\n",
    "def predictRating(user,item):\n",
    "    rating = 0\n",
    "    ratingusermean = 0\n",
    "    similarities = 0\n",
    "    rev = wordsPerUser[user]\n",
    "    pre = 0\n",
    "    if rev == [] or rev == '':\n",
    "        if itemAverages[item] == '' or itemAverages[item] == []:\n",
    "            #print('ratingMean')\n",
    "            return ratingMean\n",
    "        else:\n",
    "            #print('itemAverages')\n",
    "            return itemAverages[item]\n",
    "    \n",
    "    else:\n",
    "        ratingusermean = userAverages[user]\n",
    "        for i in wordsPerUser:\n",
    "            if i == user:\n",
    "                continue\n",
    "            else:\n",
    "                if item in itemsPerUser[i]:    \n",
    "                    documents =[wordsPerUser[user], wordsPerUser[i]]\n",
    "                    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "                    sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "                    doc_term_matrix = sparse_matrix.todense()\n",
    "                    df = pd.DataFrame(doc_term_matrix, columns=count_vectorizer.get_feature_names_out(), index=['x', 'y'])\n",
    "                    sim = cosine_similarity(df, df)[0,1]\n",
    "                    similarities += sim\n",
    "                    rr = userAverages[user]\n",
    "                    for x in reviewsPerUser[i]:\n",
    "                        if x['item_id'] == item:\n",
    "                            rr = x['rating']\n",
    "                    rating += (rr-userAverages[user]) * sim\n",
    "                    #print(user)\n",
    "        if similarities == 0:\n",
    "            pre = ratingusermean\n",
    "        else:\n",
    "            pre = ratingusermean + rating / similarities\n",
    "        if pre > 10:\n",
    "            return 10\n",
    "        else:\n",
    "            #print('pre')\n",
    "            return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "78eaa563-a54f-4d88-9389-9c1c02df6bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "square_e = 0\n",
    "step_check = 0\n",
    "for n in range(len(test)):\n",
    "    step_check += 1\n",
    "    u, i = test[n]['user_id'], test[n]['item_id']\n",
    "    predicted_rating = predictRating(u,i)\n",
    "    #print((float(test_Set[n]['rating'])),predicted_rating)\n",
    "    #print(test_Set[n]['rating'])\n",
    "    square_e += (float(test[n]['rating'])-predicted_rating)**2\n",
    "    \n",
    "    if (step_check % 1000 == 0):\n",
    "        print(step_check)\n",
    "    \n",
    "mse = square_e/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3196432a-e0f0-4f1d-b528-ed57dfaaed38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0254888390085966"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28838c7-caf0-4aec-b74a-57e4ab46847f",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "74bb7a1c-5bcf-4c99-9b27-d4926b37131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 1000 word\n",
    "words = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "950a1345-ae5f-4336-98f7-971e8ef1e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate prediction\n",
    "rate_count = defaultdict(int)\n",
    "for rate in [d['rating'] for d in training_set ]:\n",
    "    # Note = rather than +=, different versions of tf could be used instead\n",
    "    rate_count[rate] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a3cffc35-adbc-4b5b-b02c-fcd70b9e5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsListPerUser = defaultdict(str)\n",
    "for u in reviewsPerUser:\n",
    "    list_review = [review['review_text'] for review in reviewsPerUser[u]]\n",
    "    total_review = '\\n'.join(list_review)\n",
    "    reviewsListPerUser[u] = (total_review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "85b53580-6790-49c9-a657-73cec3a0096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new document frequency\n",
    "df = defaultdict(int)\n",
    "for d in reviewsListPerUser:\n",
    "    # concate words in list\n",
    "    # if we use loop for text, we can get each letter in the text\n",
    "    r = ''.join([c for c in reviewsListPerUser[d].lower() if not c in punctuation])\n",
    "    # if we find a specific word in each document, count up\n",
    "    for w in set(r.split()):\n",
    "        #df[w] += 1\n",
    "        df[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9801ce6c-d4fa-44e8-9f98-d76cf38fe029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    \n",
    "    rev = reviewsListPerUser[user]\n",
    "    if rev == [] or rev == '':\n",
    "        if itemAverages[item] == '' or itemAverages[item] == []:\n",
    "            #print('ratingMean')\n",
    "            return ratingMean\n",
    "        else:\n",
    "            #print('itemAverages')\n",
    "            return itemAverages[item]\n",
    "        \n",
    "        \n",
    "    \n",
    "    tf = defaultdict(int)\n",
    "    r = ''.join([c for c in rev.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        # Note = rather than +=, different versions of tf could be used instead\n",
    "        #tf[w] += 1\n",
    "        tf[w] = 1\n",
    "        \n",
    "    #tfidf = dict(zip(words,[tf[w] * math.log2(len(reviewsListPerUser) / df[w]) for w in words]))\n",
    "    tfidf = [tf[w] * math.log2(len(reviewsListPerUser) / df[w]) for w in words]\n",
    "\n",
    "    #print('step1 finish')\n",
    "    \n",
    "    for d in reviewsPerItem[item]:\n",
    "        i2 = d['user_id']\n",
    "        if i2 == user: continue\n",
    "        \n",
    "        tf = defaultdict(int)\n",
    "        r = ''.join([c for c in reviewsListPerUser[i2].lower() if not c in punctuation])\n",
    "        for w in r.split():\n",
    "            #tf[w] += 1\n",
    "            tf[w] = 1\n",
    "            \n",
    "        tfidf2 = [tf[w] * math.log2(len(reviewsListPerUser) / df[w]) for w in words]\n",
    "        \n",
    "        similarities.append(Cosine(tfidf, tfidf2))\n",
    "    \n",
    "    #print('step2 finish',similarities)    \n",
    "    \n",
    "    for d in reviewsPerItem[item]:\n",
    "        i2 = d['user_id']\n",
    "        if i2 == user: continue\n",
    "        # other's rating\n",
    "        ratings.append(d['rating'] - userAverages[i2])\n",
    "        \n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        pred = userAverages[user] + sum(weightedRatings) / sum(similarities)\n",
    "        if pred >= 10:\n",
    "            pred = 10\n",
    "        return pred\n",
    "    else:\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d0a0a246-341b-487d-b0c8-ccb4e48394f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {10: 6461, 6: 564, 8: 2791, 4: 127, 2: 55, 0: 2})\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "rate_count = defaultdict(int)\n",
    "for rate in [d['rating'] for d in test_set ]:\n",
    "    # Note = rather than +=, different versions of tf could be used instead\n",
    "    rate_count[rate] += 1\n",
    "\n",
    "print(rate_count)\n",
    "\n",
    "for d in test_set:\n",
    "    i = d['item_id']\n",
    "    u = d['user_id']\n",
    "    if d['rating'] is None:\n",
    "        print('None element')\n",
    "        d['rating'] = 0\n",
    "    else:\n",
    "        d['rating'] =  int(d['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8384e05c-6894-4b1f-ba00-9dbd8347e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# MSE #\n",
    "square_e = 0\n",
    "step_check = 0\n",
    "for n in range(len(test_set)):\n",
    "    step_check += 1\n",
    "    u, i = test_set[n]['user_id'], test_set[n]['item_id']\n",
    "    predicted_rating = predictRating(u,i)\n",
    "    square_e += (test_set[n]['rating']-predicted_rating)**2\n",
    "    \n",
    "    if (step_check % 1000 == 0):\n",
    "        print(step_check)\n",
    "    \n",
    "mse = square_e/len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "cb187d1f-9287-48bc-97ca-323daca4c37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.425489512540989\n"
     ]
    }
   ],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22e1a2-46f1-4606-a9c5-eae657baa668",
   "metadata": {},
   "source": [
    "#### Result\n",
    "Results of the study show that among the models listed above, the Bag of Words is the best performing model for the objective of using text-mining for rating prediction, with once again a MSE of 2.025. The results showed that using text was more beneficial than utilizing similarity with simple users and items Part of our findings indicate that users who write similar reviews also have similar preference for clothes. Thus, we can make future clothes recommendations to customers based on reviews they wrote before. It is interesting to note that the other models do not fall far behind in terms of performance and further testing and experimentation with other models mentioned as part of our recommendations could prove to yield even more accurate results. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
